{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def sigmoid(batch, stochastic=False):\n",
    "    return  1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import unicodecsv\n",
    "# from collections import namedtuple\n",
    "\n",
    "# response = []\n",
    "# f= open('final_dataset.csv', 'rb')  \n",
    "# reader = unicodecsv.DictReader(f)  \n",
    "\n",
    "# for row in reader:  \n",
    "#     response.append(row) \n",
    "# example = response[]\n",
    "# f.close()\n",
    "\n",
    "\n",
    "class StringConverter(dict):\n",
    "    def __contains__(self, item):\n",
    "        return True\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return str\n",
    "\n",
    "    def get(self, default=None):\n",
    "        return str\n",
    "pd.read_csv('final_dataset.csv', converters=StringConverter())\n",
    "data = pd.read_csv('final_dataset.csv',encoding ='unicode escape')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = pd.Series(tokens).unique().tolist()\n",
    "train_tokens = np.array([train_dict.index(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "for i in range(2,len(tokens)-2):\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-2])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+2])])\n",
    "\n",
    "train_set = np.random.permutation(np.array(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple(\"Config\", [\"dict_size\", \"vect_size\", \"neg_samples\", \"updates\", \"learning_rate\",\n",
    "                               \"learning_rate_decay\", \"decay_period\", \"log_period\"])\n",
    "conf = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=1000000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10000,\n",
    "    log_period=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(conf, train_set, train_tokens):\n",
    "    Vp = randn(conf.dict_size, conf.vect_size)\n",
    "    Vo = randn(conf.dict_size, conf.vect_size)\n",
    "\n",
    "    J = 0.0\n",
    "    learning_rate = conf.learning_rate\n",
    "    for i in range(conf.updates):\n",
    "        idx = i % len(train_set)\n",
    "\n",
    "        word = train_set[idx, 0]\n",
    "        context = train_set[idx, 1]\n",
    "\n",
    "        neg_context = np.random.randint(0, len(train_tokens), conf.neg_samples)\n",
    "        neg_context = train_tokens[neg_context]\n",
    "\n",
    "        word_vect = Vp[word, :]  # word vector\n",
    "        context_vect = Vo[context, :];  # context wector\n",
    "        negative_vects = Vo[neg_context, :]  # sampled negative vectors\n",
    "\n",
    "        # Cost and gradient calculation starts here\n",
    "        score_pos = word_vect @ context_vect.T\n",
    "        score_neg = word_vect @ negative_vects.T\n",
    "\n",
    "        J -= np.log(sigmoid(score_pos)) + np.sum(np.log(sigmoid(-score_neg)))\n",
    "        if (i + 1) % conf.log_period == 0:\n",
    "            print('Update {0}\\tcost: {1:>2.2f}'.format(i + 1, J / conf.log_period))\n",
    "            final_cost = J / conf.log_period\n",
    "            J = 0.0\n",
    "\n",
    "        pos_g = 1.0 - sigmoid(score_pos)\n",
    "        neg_g = sigmoid(score_neg)\n",
    "\n",
    "        word_grad = -pos_g * context_vect + np.sum(as_matrix(neg_g) * negative_vects, axis=0)\n",
    "        context_grad = -pos_g * word_vect\n",
    "        neg_context_grad = as_matrix(neg_g) * as_matrix(word_vect).T\n",
    "\n",
    "        Vp[word, :] -= learning_rate * word_grad\n",
    "        Vo[context, :] -= learning_rate * context_grad\n",
    "        Vo[neg_context, :] -= learning_rate * neg_context_grad\n",
    "\n",
    "        if i % conf.decay_period == 0:\n",
    "            learning_rate = learning_rate * conf.learning_rate_decay\n",
    "\n",
    "    return Vp, Vo, final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10000\tcost: 18.48\n",
      "Update 20000\tcost: 10.49\n",
      "Update 30000\tcost: 8.61\n",
      "Update 40000\tcost: 7.35\n",
      "Update 50000\tcost: 6.64\n",
      "Update 60000\tcost: 6.00\n",
      "Update 70000\tcost: 5.63\n",
      "Update 80000\tcost: 4.79\n",
      "Update 90000\tcost: 4.63\n",
      "Update 100000\tcost: 4.50\n",
      "Update 110000\tcost: 4.37\n",
      "Update 120000\tcost: 4.21\n",
      "Update 130000\tcost: 4.10\n",
      "Update 140000\tcost: 3.99\n",
      "Update 150000\tcost: 3.82\n",
      "Update 160000\tcost: 3.74\n",
      "Update 170000\tcost: 3.68\n",
      "Update 180000\tcost: 3.64\n",
      "Update 190000\tcost: 3.52\n",
      "Update 200000\tcost: 3.52\n",
      "Update 210000\tcost: 3.47\n",
      "Update 220000\tcost: 3.40\n",
      "Update 230000\tcost: 3.37\n",
      "Update 240000\tcost: 3.38\n",
      "Update 250000\tcost: 3.33\n",
      "Update 260000\tcost: 3.25\n",
      "Update 270000\tcost: 3.21\n",
      "Update 280000\tcost: 3.21\n",
      "Update 290000\tcost: 3.15\n",
      "Update 300000\tcost: 3.17\n",
      "Update 310000\tcost: 3.16\n",
      "Update 320000\tcost: 3.09\n",
      "Update 330000\tcost: 3.05\n",
      "Update 340000\tcost: 3.04\n",
      "Update 350000\tcost: 3.02\n",
      "Update 360000\tcost: 2.98\n",
      "Update 370000\tcost: 2.98\n",
      "Update 380000\tcost: 3.01\n",
      "Update 390000\tcost: 2.93\n",
      "Update 400000\tcost: 2.92\n",
      "Update 410000\tcost: 2.89\n",
      "Update 420000\tcost: 2.87\n",
      "Update 430000\tcost: 2.89\n",
      "Update 440000\tcost: 2.83\n",
      "Update 450000\tcost: 2.86\n",
      "Update 460000\tcost: 2.81\n",
      "Update 470000\tcost: 2.78\n",
      "Update 480000\tcost: 2.77\n",
      "Update 490000\tcost: 2.78\n",
      "Update 500000\tcost: 2.78\n",
      "Update 510000\tcost: 2.77\n",
      "Update 520000\tcost: 2.76\n",
      "Update 530000\tcost: 2.71\n",
      "Update 540000\tcost: 2.71\n",
      "Update 550000\tcost: 2.71\n",
      "Update 560000\tcost: 2.67\n",
      "Update 570000\tcost: 2.70\n",
      "Update 580000\tcost: 2.69\n",
      "Update 590000\tcost: 2.69\n",
      "Update 600000\tcost: 2.64\n",
      "Update 610000\tcost: 2.63\n",
      "Update 620000\tcost: 2.63\n",
      "Update 630000\tcost: 2.62\n",
      "Update 640000\tcost: 2.61\n",
      "Update 650000\tcost: 2.64\n",
      "Update 660000\tcost: 2.59\n",
      "Update 670000\tcost: 2.57\n",
      "Update 680000\tcost: 2.58\n",
      "Update 690000\tcost: 2.57\n",
      "Update 700000\tcost: 2.55\n",
      "Update 710000\tcost: 2.55\n",
      "Update 720000\tcost: 2.60\n",
      "Update 730000\tcost: 2.54\n",
      "Update 740000\tcost: 2.53\n",
      "Update 750000\tcost: 2.51\n",
      "Update 760000\tcost: 2.52\n",
      "Update 770000\tcost: 2.54\n",
      "Update 780000\tcost: 2.51\n",
      "Update 790000\tcost: 2.54\n",
      "Update 800000\tcost: 2.49\n",
      "Update 810000\tcost: 2.47\n",
      "Update 820000\tcost: 2.46\n",
      "Update 830000\tcost: 2.47\n",
      "Update 840000\tcost: 2.48\n",
      "Update 850000\tcost: 2.47\n",
      "Update 860000\tcost: 2.49\n",
      "Update 870000\tcost: 2.44\n",
      "Update 880000\tcost: 2.43\n",
      "Update 890000\tcost: 2.43\n",
      "Update 900000\tcost: 2.43\n",
      "Update 910000\tcost: 2.44\n",
      "Update 920000\tcost: 2.45\n",
      "Update 930000\tcost: 2.44\n",
      "Update 940000\tcost: 2.41\n",
      "Update 950000\tcost: 2.40\n",
      "Update 960000\tcost: 2.40\n",
      "Update 970000\tcost: 2.41\n",
      "Update 980000\tcost: 2.40\n",
      "Update 990000\tcost: 2.43\n",
      "Update 1000000\tcost: 2.41\n"
     ]
    }
   ],
   "source": [
    "Vp, Vo, J = neg_sample(conf, train_set, train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)    \n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training cost: 2.41\n",
      "\n",
      "\n",
      "Words similar to good: good, unclog, purest, air, CONCORDE\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print('\\n\\nTraining cost: {0:>2.2f}\\n\\n'.format(J))\n",
    "\n",
    "sample_words = [ 'good']\n",
    "\n",
    "Vp_norm = Vp / as_matrix(np.linalg.norm(Vp , axis=1))\n",
    "for w in sample_words:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print('Words similar to {}: {}'.format(w, \", \".join(similar)))\n",
    "\n",
    "    #word_dic issue not fixed, not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References\n",
    "#[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
